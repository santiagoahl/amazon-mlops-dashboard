{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIDA HP\n"
     ]
    }
   ],
   "source": [
    "\"This script computes inferences\"\n",
    "\"\"\"Python script to train Tennis Demmand models\"\"\"\n",
    "\n",
    "# Utils\n",
    "from config import Location, ModelParams\n",
    "from prefect import flow, task\n",
    "from datetime import date, datetime\n",
    "from typing import Union, Any\n",
    "import logging\n",
    "\n",
    "# Data Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import data_preprocessing as dp\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "import mlflow\n",
    "\n",
    "# System\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "\n",
    "TIMESTAMP = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "DEV_NAME = \"santiagoal\"\n",
    "OUTPUT_VAR_NAME = \"sales_volume\"\n",
    "\n",
    "mlflow_client = mlflow.MlflowClient(tracking_uri=Location().mlflow_tracking_uri)\n",
    "os.environ[\"LOGNAME\"] = DEV_NAME\n",
    "\n",
    "# Set Logger\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "    \n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s:%(name)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y:%m:%d %H:%M:%S\",\n",
    "    filename=os.path.join(Location().root_dir, \"data/logs/inference/output.log\")\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(name=\"Logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def import_data(location: Location = Location()) -> pd.DataFrame:   \n",
    "    \"\"\"\n",
    "    Description.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1 : type\n",
    "        Description\n",
    "    arg2 : type\n",
    "        Description\n",
    "    arg3 : type\n",
    "        Description\n",
    "\n",
    "    Returns:\n",
    "        type:\n",
    "    \n",
    "    Example:\n",
    "        >>> ('arg1', 'arg2')\n",
    "        'output'\n",
    "    \"\"\"\n",
    "    df_raw = dp.extract_json_df()\n",
    "    return df_raw\n",
    "\n",
    "\n",
    "##@task\n",
    "def prepare_data(df: pd.DataFrame = None, location: Location = Location()) -> pd.DataFrame:   \n",
    "    \"\"\"\n",
    "    Description.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1 : type\n",
    "        Description\n",
    "    arg2 : type\n",
    "        Description\n",
    "    arg3 : type\n",
    "        Description\n",
    "\n",
    "    Returns:\n",
    "        type:\n",
    "    \n",
    "    Example:\n",
    "        >>> ('arg1', 'arg2')\n",
    "        'output'\n",
    "    \"\"\"\n",
    "    df_clean = dp.clean_data(df)\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "##@task\n",
    "def compute_inferences(input_df: Union[pd.DataFrame, np.array], location: Location = Location()) -> Any:   \n",
    "    \"\"\"\n",
    "    Description.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1 : type\n",
    "        Description\n",
    "    arg2 : type\n",
    "        Description\n",
    "    arg3 : type\n",
    "        Description\n",
    "\n",
    "    Returns:\n",
    "        type:\n",
    "    \n",
    "    Example:\n",
    "        >>> ('arg1', 'arg2')\n",
    "        'output'\n",
    "    \"\"\"\n",
    "   \n",
    "    # Import model\n",
    "    with open(location.model, \"rb\") as m:\n",
    "        model = joblib.load(m)\n",
    "    \n",
    "    input_df.drop(\"sales_volume\", axis=1, inplace=True)\n",
    "    # Compute compute_inferences\n",
    "    predictions = model.predict(input_df)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "##@task\n",
    "def include_predictions(\n",
    "            input_df: Union[pd.DataFrame, np.array], \n",
    "            predictions: np.array, \n",
    "            location: Location = Location()\n",
    "        ) -> pd.DataFrame:   \n",
    "    \"\"\"\n",
    "    Description.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1 : type\n",
    "        Description\n",
    "    arg2 : type\n",
    "        Description\n",
    "    arg3 : type\n",
    "        Description\n",
    "\n",
    "    Returns:\n",
    "        type:\n",
    "    \n",
    "    Example:\n",
    "        >>> ('arg1', 'arg2')\n",
    "        'output'\n",
    "    \"\"\"\n",
    "    output_df = input_df.copy()\n",
    "    output_df[OUTPUT_VAR_NAME] = predictions\n",
    "    output_df[OUTPUT_VAR_NAME] = output_df[OUTPUT_VAR_NAME].map(lambda x: int(x))  # Convert to integers\n",
    "    ordered_cols = [col for col in output_df.columns if col != OUTPUT_VAR_NAME] + [OUTPUT_VAR_NAME]\n",
    "    output_df = output_df[ordered_cols]\n",
    "    output_df.to_csv(location.data_final, index=False)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "#@flow\n",
    "def predict(location: Location = Location()) -> None:   \n",
    "    \"\"\"\n",
    "    Description.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1 : type\n",
    "        Description\n",
    "    arg2 : type\n",
    "        Description\n",
    "    arg3 : type\n",
    "        Description\n",
    "\n",
    "    Returns:\n",
    "        type:\n",
    "    \n",
    "    Example:\n",
    "        >>> ('arg1', 'arg2')\n",
    "        'output'\n",
    "    \"\"\"\n",
    "    logger.info(\"Importing Data...\")\n",
    "    #df_raw = import_data()\n",
    "    logger.info(\"Preparing Data for Inferences...\")\n",
    "    #input_df = prepare_data(df=df_raw)\n",
    "    logger.info(\"Computing Inferences...\")\n",
    "    #predictions = compute_inferences(input_df=input_df)\n",
    "    logger.info(\"Saving compute_inferences...\")\n",
    "    #include_predictions(input_df=df_raw, predictions=predictions)\n",
    "    print(\"VIDA HP\")\n",
    "    logger.info(f\"Inferences Succesfully Computed. \\ncompute_inferences Saved in {location.data_final}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"XD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
