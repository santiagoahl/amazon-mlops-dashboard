# TODO

## Milestone 1: Get data

* [X] Get a scope of which data can be tracked

  * [X] Try out with 10 different api requests
  * [X] Save the outputs in `data/raw/`
* [ ] KDE data augmentation

  * [ ] Create an script for data augmentation -> take care of variables distribution
  * [ ] Test the script into a notebook and save the data (I would like to have ~50k datapoints)
  * [ ] Make a simple sketch of how a data transformation pipeline would work, let's try out with figma
* [ ] Mount data in Databricks

  * [ ] Mount data
  * [ ] Try to manage using spark
  * [ ] Connect MLFLOW
* [ ] Experiment with MLFLOW

  * [ ] Focus on linear models and feature engineering
  * [ ] Try to achieve good r2 and rmse metrics
  * [ ] Visualize model performance results using cool plots
  * [ ] How to reproduce experiments?
* [ ] Learn DataBricks

  * [ ] Interact with databricks
* [ ] Connect repo to aws

# Milestone 2: MLFlow Experimentation

* [ ] Try out with XGB, LGBM
